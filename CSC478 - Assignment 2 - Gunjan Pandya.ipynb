{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\GunjanPandya\\\\Downloads'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GunjanPandya\\Downloads\\CSC478\\newsgroups\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\GunjanPandya\\Downloads\\CSC478\\newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. K-Nearest-Neighbor (KNN) classification on Newsgroups [Dataset: newsgroups.zip]\n",
    "For this problem you will use a subset of the 20 Newsgroup data set. The full data set contains 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups and has been often used for experiments in text applications of machine learning techniques, such as text classification and text clustering (see the description of the full dataset). The assignment data set contains a subset of 1000 documents and a vocabulary of  terms. Each document belongs to one of two classes Hockey (class label 1) and Microsoft Windows (class label 0). The data has already been split (80%, 20%) into training and test data. The class labels for the training and test data are also provided in separate files. The training and test data contain a row for each term in the vocabulary and a column for each document. The values in the table represent raw term frequencies. The data has already been preprocessed to extract terms, remove stop words and perform stemming (so, the vocabulary contains stems not full terms). Please be sure to read the readme.txt file in the distribution.\n",
    "\n",
    "Your tasks in this problem are the following [Note: for this problem you should not use scikit-learn or other external libraries other than Pandas, NumPy, standard Python libraries, and Matplotlib (if you would like to add some visualizations to your answers.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "\n",
    "train_data = pd.read_table('trainMatrixModified.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>790</th>\n",
       "      <th>791</th>\n",
       "      <th>792</th>\n",
       "      <th>793</th>\n",
       "      <th>794</th>\n",
       "      <th>795</th>\n",
       "      <th>796</th>\n",
       "      <th>797</th>\n",
       "      <th>798</th>\n",
       "      <th>799</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 800 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9   ...   790  791  792  793  \\\n",
       "0  2.0  0.0  0.0  2.0  2.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n",
       "1  2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  3.0  0.0   \n",
       "2  2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n",
       "3  1.0  1.0  1.0  1.0  1.0  1.0  2.0  1.0  1.0  1.0 ...   1.0  1.0  1.0  1.0   \n",
       "4  8.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  2.0  0.0   \n",
       "\n",
       "   794  795  796  797  798  799  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "\n",
       "[5 rows x 800 columns]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5500, 800)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_table('testMatrixModified.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9   ...   190  191  192  193  \\\n",
       "0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n",
       "3  1.0  1.0  1.0  1.0  1.0  1.0  2.0  1.0  1.0  1.0 ...   1.0  2.0  1.0  1.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0 ...   0.0  0.0  0.0  0.0   \n",
       "\n",
       "   194  195  196  197  198  199  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5500, 200)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = pd.read_table('modifiedterms.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>david</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>call</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0\n",
       "0    david\n",
       "1      rex\n",
       "2     wood\n",
       "3  subject\n",
       "4     call"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5500, 1)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_classes = pd.read_table('trainClasses.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  0  0\n",
       "1  1  1\n",
       "2  2  0\n",
       "3  3  1\n",
       "4  4  0"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1\n",
       "0  0\n",
       "1  1\n",
       "2  0\n",
       "3  1\n",
       "4  0"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = train_classes.drop(train_classes.columns[0], axis=1)\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_classes = pd.read_table('testClasses.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  0  1\n",
       "1  1  0\n",
       "2  2  0\n",
       "3  3  1\n",
       "4  4  1"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_classes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1\n",
       "0    1\n",
       "1    0\n",
       "2    0\n",
       "3    1\n",
       "4    1\n",
       "5    0\n",
       "6    1\n",
       "7    1\n",
       "8    0\n",
       "9    1\n",
       "10   0\n",
       "11   1\n",
       "12   0\n",
       "13   0\n",
       "14   1\n",
       "15   1\n",
       "16   0\n",
       "17   1\n",
       "18   0\n",
       "19   0\n",
       "20   0\n",
       "21   0\n",
       "22   0\n",
       "23   0\n",
       "24   1\n",
       "25   1\n",
       "26   1\n",
       "27   0\n",
       "28   1\n",
       "29   1\n",
       "..  ..\n",
       "170  1\n",
       "171  1\n",
       "172  1\n",
       "173  0\n",
       "174  1\n",
       "175  1\n",
       "176  0\n",
       "177  1\n",
       "178  0\n",
       "179  1\n",
       "180  1\n",
       "181  1\n",
       "182  1\n",
       "183  0\n",
       "184  1\n",
       "185  0\n",
       "186  1\n",
       "187  0\n",
       "188  1\n",
       "189  1\n",
       "190  0\n",
       "191  0\n",
       "192  0\n",
       "193  0\n",
       "194  0\n",
       "195  1\n",
       "196  1\n",
       "197  0\n",
       "198  1\n",
       "199  1\n",
       "\n",
       "[200 rows x 1 columns]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = test_classes.drop(test_classes.columns[0], axis=1)\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#converting from TD to DT matrix\n",
    "\n",
    "DT_train = train_data.T\n",
    "DT_test = test_data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5490</th>\n",
       "      <th>5491</th>\n",
       "      <th>5492</th>\n",
       "      <th>5493</th>\n",
       "      <th>5494</th>\n",
       "      <th>5495</th>\n",
       "      <th>5496</th>\n",
       "      <th>5497</th>\n",
       "      <th>5498</th>\n",
       "      <th>5499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows Ã— 5500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4     5     6     7     8     9     ...   5490  \\\n",
       "0     2.0   2.0   2.0   1.0   8.0   6.0   2.0   8.0   2.0   4.0  ...    0.0   \n",
       "1     0.0   0.0   0.0   1.0   1.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "2     0.0   0.0   0.0   1.0   0.0   0.0   0.0   2.0   0.0   1.0  ...    0.0   \n",
       "3     2.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "4     2.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "5     0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "6     0.0   0.0   0.0   2.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "7     0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   2.0  ...    0.0   \n",
       "8     0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "9     0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "10    0.0   0.0   0.0   1.0   1.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "11    0.0   0.0   0.0   2.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "12    0.0   0.0   0.0   3.0   0.0   0.0   1.0   0.0   0.0   0.0  ...    0.0   \n",
       "13    0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "14    0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "15    0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   2.0  ...    0.0   \n",
       "16    0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "17    0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0  ...    0.0   \n",
       "18    0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "19    0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0  ...    0.0   \n",
       "20    0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "21    0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0  ...    0.0   \n",
       "22    0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "23    0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "24    0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "25    0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "26    0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "27    0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "28    0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "29    0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
       "770   0.0   0.0   0.0   1.0   2.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "771   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "772   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   2.0  ...    0.0   \n",
       "773   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "774   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0  ...    0.0   \n",
       "775   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0  ...    0.0   \n",
       "776   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "777   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "778   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "779   4.0   0.0   0.0  36.0  33.0   5.0  17.0  37.0   3.0   8.0  ...    0.0   \n",
       "780   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "781   0.0   0.0   0.0   1.0   0.0   0.0   0.0   2.0   0.0   0.0  ...    0.0   \n",
       "782   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "783   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "784   0.0   0.0   0.0   1.0   0.0   1.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "785   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "786   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0  ...    0.0   \n",
       "787   0.0   0.0   0.0   1.0   1.0   0.0   0.0   0.0   0.0   2.0  ...    0.0   \n",
       "788   0.0   0.0   0.0   1.0   5.0   0.0   0.0   0.0   0.0   1.0  ...    0.0   \n",
       "789   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "790   0.0   0.0   0.0   1.0   0.0   0.0   2.0   2.0   0.0   0.0  ...    0.0   \n",
       "791   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "792   0.0   3.0   0.0   1.0   2.0   0.0   0.0   0.0   0.0   1.0  ...    0.0   \n",
       "793   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "794   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0  ...    0.0   \n",
       "795   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "796   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "797   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "798   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "799   0.0   0.0   0.0   1.0   1.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "\n",
       "     5491  5492  5493  5494  5495  5496  5497  5498  5499  \n",
       "0     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "5     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "6     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "7     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "8     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "9     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "10    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "11    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "12    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "13    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "14    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "15    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "16    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "17    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "18    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "19    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "20    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "21    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "22    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "23    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "24    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "25    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "26    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "27    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "28    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "29    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "770   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "771   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "772   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "773   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "774   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "775   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "776   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "777   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "778   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "779   0.0   0.0   1.0   0.0   1.0   0.0   0.0   0.0   0.0  \n",
       "780   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "781   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "782   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "783   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "784   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0  \n",
       "785   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0  \n",
       "786   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "787   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "788   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "789   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "790   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "791   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "792   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "793   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "794   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "795   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "796   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "797   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "798   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "799   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[800 rows x 5500 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('00', 159.0), ('10', 254.0), ('1010', 105.0), ('125', 74.0), ('14', 100.0), ('1990', 73.0), ('1991', 159.0), ('1992', 23.0), ('1993', 80.0), ('1st', 67.0), ('20', 32.0), ('21', 61.0), ('25', 136.0), ('2nd', 24.0), ('30', 59.0), ('4324219', 24.0), ('50', 78.0), ('5000', 286.0), ('63', 44.0), ('635', 42.0), ('705', 78.0), ('89', 204.0), ('919', 113.0), ('999', 32.0), ('abc', 172.0), ('abl', 73.0), ('accept', 195.0), ('access', 53.0), ('act', 168.0), ('activ', 107.0), ('actual', 485.0), ('adam', 81.0), ('add', 205.0), ('addit', 53.0), ('address', 31.0), ('admit', 81.0), ('admittedli', 202.0), ('advanc', 125.0), ('advantag', 45.0), ('advic', 75.0), ('afternoon', 102.0), ('ago', 78.0), ('allstar', 47.0), ('altern', 2.0), ('amount', 330.0), ('analyst', 51.0), ('anderson', 99.0), ('andrewcmuedu', 28.0), ('angel', 253.0), ('angelo', 72.0), ('announc', 48.0), ('anonym', 28.0), ('anybodi', 89.0), ('anymor', 89.0), ('anywai', 792.0), ('app', 50.0), ('appear', 214.0), ('appropri', 74.0), ('archiv', 169.0), ('area', 109.0), ('argument', 7.0), ('arrang', 2754.0), ('articl', 75.0), ('audett', 59.0), ('automat', 79.0), ('avail', 51.0), ('averag', 28.0), ('awai', 41.0), ('back', 22.0), ('bad', 89.0), ('bai', 97.0), ('baker', 119.0), ('balanc', 50.0), ('base', 108.0), ('basic', 114.0), ('bassen', 66.0), ('beat', 109.0), ('behind', 96.0), ('belfour', 79.0), ('bell', 23.0), ('berth', 142.0), ('best', 44.0), ('better', 122.0), ('biggest', 48.0), ('bill', 29.0), ('binari', 45.0), ('blackhawk', 57.0), ('blame', 45.0), ('blow', 82.0), ('blown', 225.0), ('blue', 110.0), ('bodi', 43.0), ('bondra', 72.0), ('bonehead', 35.0), ('boora', 79.0), ('boston', 59.0), ('boulder', 48.0), ('bounc', 46.0), ('bourqu', 244.0), ('box', 39.0), ('bradlei', 27.0), ('brian', 99.0), ('broadcast', 67.0), ('broten', 84.0), ('brought', 51.0), ('bruin', 74.0), ('brutal', 104.0), ('bryan', 49.0), ('buchberg', 67.0), ('buffalo', 80.0), ('buffer', 53.0), ('bure', 209.0), ('busi', 40.0), ('button', 11.0), ('calgari', 58.0), ('call', 33.0), ('canada', 55.0), ('canadian', 40.0), ('canadien', 1465.0), ('canuck', 49.0), ('cap', 75.0), ('capit', 89.0), ('care', 180.0), ('carri', 49.0), ('cassel', 76.0), ('caught', 48.0), ('cbc', 145.0), ('ccdb', 126.0), ('cha', 75.0), ('chaisson', 55.0), ('champion', 53.0), ('chanc', 102.0), ('chang', 88.0), ('cheap', 82.0), ('cheaper', 82.0), ('check', 101.0), ('chest', 49.0), ('chicago', 50.0), ('choic', 70.0), ('chri', 219.0), ('class', 105.0), ('clement', 51.0), ('click', 40.0), ('close', 32.0), ('closer', 127.0), ('coach', 13.0), ('code', 55.0), ('colorado', 190.0), ('come', 6.0), ('comeback', 48.0), ('command', 90.0), ('competit', 127.0), ('compil', 115.0), ('conach', 113.0), ('connect', 74.0), ('content', 93.0), ('contribut', 93.0), ('contributor', 71.0), ('control', 434.0), ('convent', 68.0), ('cote', 116.0), ('cours', 52.0), ('cover', 38.0), ('creat', 119.0), ('creighton', 56.0), ('crosscheck', 46.0), ('cscoloradoedu', 86.0), ('cup', 32.0), ('current', 14.0), ('cut', 48.0), ('dahlquist', 139.0), ('dai', 109.0), ('dan', 102.0), ('date', 60.0), ('david', 196.0), ('davidson', 228.0), ('dead', 34.0), ('dec', 39.0), ('decis', 59.0), ('decstat', 75.0), ('decwindow', 52.0), ('deep', 32.0), ('defens', 44.0), ('demand', 134.0), ('demer', 90.0), ('demo', 87.0), ('demonstr', 48.0), ('deni', 132.0), ('denni', 22.0), ('desktop', 200.0), ('desper', 93.0), ('despit', 171.0), ('detroit', 123.0), ('devil', 46.0), ('dg', 140.0), ('di', 14.0), ('differ', 110.0), ('difficulti', 73.0), ('disappoint', 94.0), ('discuss', 58.0), ('displai', 68.0), ('divis', 51.0), ('domin', 45.0), ('done', 25.0), ('donnelli', 87.0), ('doubl', 66.0), ('doublehead', 115.0), ('doubt', 130.0), ('dress', 47.0), ('drop', 109.0), ('druce', 67.0), ('dsweenei', 356.0), ('due', 121.0), ('dump', 34.0), ('easili', 142.0), ('east', 513.0), ('ed', 32.0), ('edit', 86.0), ('edmonton', 25.0), ('eklund', 28.0), ('elbow', 243.0), ('ellett', 119.0), ('elynuik', 14.0), ('email', 141.0), ('encourag', 127.0), ('end', 60.0), ('enjoi', 29.0), ('entertain', 15.0), ('error', 69.0), ('especi', 62.0), ('espn', 69.0), ('establish', 12.0), ('event', 68.0), ('everyon', 37.0), ('everyth', 103.0), ('exampl', 162.0), ('except', 36.0), ('exit', 134.0), ('expect', 167.0), ('explor', 34.0), ('extens', 67.0), ('fact', 55.0), ('fail', 45.0), ('fan', 220.0), ('faq', 20.0), ('far', 78.0), ('fat', 64.0), ('fax', 151.0), ('fedyk', 72.0), ('feed', 115.0), ('felt', 30.0), ('figur', 94.0), ('file', 41.0), ('final', 57.0), ('finish', 38.0), ('first', 32.0), ('five', 28.0), ('fix', 220.0), ('flame', 104.0), ('fleuri', 97.0), ('flyer', 52.0), ('folk', 124.0), ('follow', 115.0), ('forget', 68.0), ('form', 70.0), ('format', 135.0), ('former', 54.0), ('forum', 118.0), ('freenetcarletonca', 113.0), ('frustrat', 33.0), ('ftp', 74.0), ('fuhr', 30.0), ('fulli', 26.0), ('fun', 28.0), ('function', 94.0), ('gallei', 52.0), ('game', 28.0), ('gari', 48.0), ('gaudreau', 64.0), ('gaussmedharvardedu', 35.0), ('gener', 56.0), ('get', 269.0), ('ghostscript', 29.0), ('gibson', 35.0), ('gilmour', 114.0), ('give', 76.0), ('given', 175.0), ('glenn', 28.0), ('gnu', 103.0), ('go', 197.0), ('goal', 112.0), ('goaltend', 100.0), ('godfath', 72.0), ('goe', 705.0), ('good', 113.0), ('got', 25.0), ('gotta', 93.0), ('goulet', 82.0), ('grab', 25.0), ('grant', 54.0), ('great', 38.0), ('gretzki', 123.0), ('group', 215.0), ('guess', 33.0), ('gui', 536.0), ('hab', 87.0), ('half', 74.0), ('hardli', 127.0), ('harri', 27.0), ('hartford', 55.0), ('hatcher', 77.0), ('have', 142.0), ('hawk', 53.0), ('head', 17.0), ('header', 73.0), ('headlin', 111.0), ('healthyuwaterlooca', 119.0), ('heck', 30.0), ('hei', 520.0), ('height', 72.0), ('help', 16.0), ('hextal', 90.0), ('hi', 84.0), ('hierarchi', 199.0), ('highstick', 119.0), ('histori', 48.0), ('hit', 293.0), ('hockei', 4367.0), ('hope', 50.0), ('host', 26.0), ('hot', 140.0), ('hrudei', 59.0), ('hsdndevharvardedu', 68.0), ('ic', 124.0), ('icon', 27.0), ('idea', 2715.0), ('imho', 102.0), ('impal', 254.0), ('impress', 137.0), ('improv', 337.0), ('includ', 124.0), ('inclus', 88.0), ('increas', 57.0), ('inform', 45.0), ('initi', 69.0), ('insert', 49.0), ('instal', 70.0), ('institut', 248.0), ('int', 35.0), ('integr', 55.0), ('interact', 94.0), ('interfac', 25.0), ('internet', 26.0), ('involv', 78.0), ('ipc', 82.0), ('isl', 122.0), ('island', 73.0), ('jacqu', 134.0), ('jagr', 48.0), ('jbrown', 95.0), ('jersei', 20.0), ('jet', 31.0), ('johansson', 64.0), ('john', 76.0), ('jose', 111.0), ('june', 71.0), ('just', 94.0), ('keep', 26.0), ('kept', 183.0), ('kick', 41.0), ('king', 17.0), ('kisio', 66.0), ('kitssfuca', 297.0), ('know', 54.0), ('kovalenko', 161.0), ('kovalev', 61.0), ('kozlov', 70.0), ('la', 32.0), ('laboratori', 49.0), ('lack', 148.0), ('ladi', 31.0), ('lafontain', 119.0), ('lead', 106.0), ('leadership', 63.0), ('leaf', 45.0), ('least', 61.0), ('lebeau', 44.0), ('left', 206.0), ('lemieux', 47.0), ('level', 431.0), ('lib', 236.0), ('librari', 245.0), ('linden', 144.0), ('lindro', 53.0), ('line', 35.0), ('link', 216.0), ('list', 13.0), ('littl', 33.0), ('lo', 96.0), ('load', 47.0), ('local', 17.0), ('long', 57.0), ('look', 36.0), ('lose', 73.0), ('loss', 310.0), ('lost', 37.0), ('loui', 135.0), ('love', 92.0), ('maciv', 68.0), ('maclean', 44.0), ('magazin', 447.0), ('mail', 195.0), ('main', 70.0), ('major', 47.0), ('makarov', 75.0), ('manag', 131.0), ('manson', 40.0), ('mapl', 31.0), ('mark', 168.0), ('match', 110.0), ('mclean', 66.0), ('mean', 98.0), ('mechan', 44.0), ('mee', 45.0), ('mellanbi', 105.0), ('member', 28.0), ('mention', 161.0), ('messag', 143.0), ('messier', 87.0), ('min', 44.0), ('mind', 33.0), ('minnesota', 11.0), ('minor', 36.0), ('minut', 49.0), ('misconduct', 9.0), ('miss', 185.0), ('mit', 58.0), ('mo', 778.0), ('modano', 16.0), ('model', 58.0), ('moder', 4.0), ('mogilni', 56.0), ('momesso', 22.0), ('montreal', 129.0), ('moog', 217.0), ('morn', 46.0), ('move', 136.0), ('muller', 64.0), ('multipl', 62.0), ('musicamcgillca', 19.0), ('mvp', 98.0), ('name', 47.0), ('nation', 42.0), ('natur', 28.0), ('ncd', 46.0), ('necessarili', 58.0), ('nedv', 17.0), ('need', 357.0), ('neglect', 244.0), ('network', 21.0), ('new', 14.0), ('newsdisplai', 27.0), ('newsgroup', 42.0), ('newspap', 118.0), ('nhl', 205.0), ('nhma', 24.0), ('night', 95.0), ('nord', 227.0), ('nordiqu', 40.0), ('north', 31.0), ('noth', 302.0), ('notic', 82.0), ('null', 128.0), ('ny', 68.0), ('oat', 70.0), ('observ', 101.0), ('obtain', 195.0), ('odd', 63.0), ('offenc', 61.0), ('offens', 70.0), ('offer', 56.0), ('oh', 303.0), ('oiler', 187.0), ('ok', 28.0), ('okai', 57.0), ('old', 203.0), ('on', 81.0), ('ontario', 151.0), ('open', 60.0), ('opinion', 51.0), ('opportun', 24.0), ('option', 64.0), ('ot', 26.0), ('ottawa', 173.0), ('outlin', 74.0), ('output', 31.0), ('outset', 75.0), ('outsid', 65.0), ('pair', 55.0), ('particip', 172.0), ('particular', 35.0), ('past', 152.0), ('pat', 27.0), ('patch', 57.0), ('pathet', 68.0), ('patrick', 177.0), ('paul', 72.0), ('pearson', 95.0), ('pen', 21.0), ('penalti', 63.0), ('penguin', 50.0), ('peopl', 89.0), ('perform', 27.0), ('period', 13.0), ('petit', 55.0), ('pgh', 91.0), ('philadelphia', 21.0), ('phone', 17.0), ('physic', 67.0), ('pick', 217.0), ('pie', 40.0), ('pine', 40.0), ('pittsburgh', 61.0), ('place', 58.0), ('plai', 47.0), ('plan', 30.0), ('player', 68.0), ('playoff', 44.0), ('pleas', 31.0), ('pleshar', 99.0), ('plu', 109.0), ('po', 28.0), ('point', 132.0), ('polici', 72.0), ('possibl', 71.0), ('post', 413.0), ('postscript', 65.0), ('potvin', 29.0), ('ppv', 17.0), ('previou', 84.0), ('printf', 366.0), ('probabl', 108.0), ('problem', 21.0), ('procedur', 15.0), ('product', 3740.0), ('prog', 25.0), ('program', 89.0), ('programm', 62.0), ('propag', 116.0), ('proper', 70.0), ('prove', 154.0), ('provid', 298.0), ('psuvmpsuedu', 71.0), ('publish', 135.0), ('pull', 73.0), ('punch', 69.0), ('put', 55.0), ('quebec', 127.0), ('question', 95.0), ('r4', 226.0), ('r5', 36.0), ('rais', 42.0), ('ralph', 182.0), ('ranger', 43.0), ('rap115', 19.0), ('reader', 105.0), ('real', 51.0), ('realli', 48.0), ('recchi', 1051.0), ('receiv', 131.0), ('record', 54.0), ('recov', 68.0), ('red', 232.0), ('reichel', 318.0), ('rel', 101.0), ('relat', 33.0), ('rememb', 147.0), ('repres', 155.0), ('request', 25.0), ('requir', 31.0), ('resembl', 60.0), ('resourc', 92.0), ('respect', 13.0), ('respond', 63.0), ('rest', 33.0), ('return', 64.0), ('rex', 79.0), ('ricci', 133.0), ('right', 101.0), ('ring', 77.0), ('rm', 268.0), ('robbi', 101.0), ('robert', 38.0), ('robitail', 121.0), ('roenick', 22.0), ('roger', 46.0), ('roi', 46.0), ('ron', 78.0), ('room', 57.0), ('rose', 36.0), ('round', 122.0), ('rout', 81.0), ('rp16', 24.0), ('rub', 37.0), ('run', 52.0), ('rush', 129.0), ('ruuttu', 130.0), ('rychel', 27.0), ('sa', 94.0), ('sabr', 115.0), ('sai', 29.0), ('sakic', 44.0), ('san', 57.0), ('sanderson', 62.0), ('saturdai', 66.0), ('savard', 37.0), ('save', 46.0), ('saw', 117.0), ('schedul', 24.0), ('sco', 93.0), ('score', 155.0), ('scott', 7.0), ('screen', 17.0), ('screw', 54.0), ('season', 77.0), ('second', 109.0), ('see', 86.0), ('seen', 187.0), ('selann', 27.0), ('select', 107.0), ('semak', 31.0), ('semi', 194.0), ('senat', 34.0), ('send', 174.0), ('sergei', 235.0), ('seri', 237.0), ('seriou', 128.0), ('serv', 123.0), ('set', 49.0), ('setup', 58.0), ('sfuca', 80.0), ('shanahan', 157.0), ('shark', 92.0), ('shift', 156.0), ('shneyder', 36.0), ('shot', 268.0), ('show', 57.0), ('similarli', 160.0), ('sit', 71.0), ('site', 48.0), ('sleep', 101.0), ('smale', 43.0), ('softwar', 32.0), ('someth', 62.0), ('sourc', 30.0), ('southern', 93.0), ('speak', 86.0), ('specif', 86.0), ('st', 78.0), ('stanlei', 146.0), ('star', 47.0), ('start', 84.0), ('static', 3468.0), ('stdioh', 94.0), ('steven', 38.0), ('stloui', 96.0), ('straight', 42.0), ('stride', 30.0), ('strong', 66.0), ('stronger', 99.0), ('strongli', 109.0), ('stuff', 5.0), ('subject', 39.0), ('submiss', 80.0), ('subscrib', 34.0), ('subscript', 74.0), ('suggest', 25.0), ('summer', 216.0), ('sun', 191.0), ('sundai', 39.0), ('superiorcarletonca', 166.0), ('support', 28.0), ('surpris', 32.0), ('suspens', 82.0), ('suter', 124.0), ('sutter', 105.0), ('symasussexacuk', 60.0), ('system', 194.0), ('take', 1464.0), ('talent', 91.0), ('tampa', 60.0), ('team', 41.0), ('technolog', 770.0), ('tee', 128.0), ('tell', 41.0), ('term', 1702.0), ('termin', 16.0), ('test', 94.0), ('thank', 46.0), ('thing', 41.0), ('think', 57.0), ('third', 41.0), ('thorn', 52.0), ('thought', 42.0), ('three', 49.0), ('thursdai', 97.0), ('time', 219.0), ('tocchet', 23.0), ('todai', 55.0), ('told', 55.0), ('tom', 21.0), ('tonight', 199.0), ('top', 15.0), ('toronto', 43.0), ('total', 85.0), ('tough', 31.0), ('tree', 87.0), ('tri', 194.0), ('troubl', 136.0), ('truli', 61.0), ('try', 51.0), ('tuesdai', 56.0), ('turgeon', 22.0), ('tv', 187.0), ('twm', 131.0), ('two', 143.0), ('unabl', 132.0), ('unfortun', 25.0), ('univers', 152.0), ('unix', 39.0), ('unusu', 104.0), ('upcom', 162.0), ('updat', 159.0), ('us', 6.0), ('usenet', 122.0), ('usual', 99.0), ('uucp', 145.0), ('uunetuunet', 98.0), ('vacat', 257.0), ('valuabl', 49.0), ('vancouv', 34.0), ('var', 180.0), ('variabl', 21.0), ('variant', 84.0), ('vaxcnsmuskingumedu', 28.0), ('vernon', 47.0), ('view', 83.0), ('vlad', 101.0), ('vladimir', 63.0), ('void', 55.0), ('vzhivov', 145.0), ('wai', 75.0), ('wait', 74.0), ('want', 40.0), ('warren', 27.0), ('washington', 54.0), ('watch', 31.0), ('wednesdai', 60.0), ('weight', 121.0), ('welcom', 66.0), ('went', 111.0), ('weslei', 258.0), ('west', 213.0), ('whaler', 62.0), ('widget', 55.0), ('win', 51.0), ('window', 106.0), ('wing', 64.0), ('winnipeg', 180.0), ('wm', 154.0), ('wood', 51.0), ('word', 40.0), ('work', 50.0), ('worldwid', 50.0), ('worst', 77.0), ('write', 82.0), ('writer', 38.0), ('wrong', 14.0), ('wrote', 129.0), ('x11', 37.0), ('x11r5', 61.0), ('xdm', 125.0), ('xevent', 53.0), ('xlibh', 47.0), ('xm', 168.0), ('xmh', 665.0), ('xtappcontext', 73.0), ('xtappiniti', 90.0), ('xtdestroywidget', 85.0), ('xtdispatchev', 89.0), ('xterm', 1477.0), ('xtermin', 64.0), ('xtrealizewidget', 95.0), ('xtvacreatemanagedwidget', 108.0), ('yawnei', 42.0), ('year', 92.0), ('york', 34.0), ('yzerman', 38.0), ('zhivov', 183.0), ('zone', 51.0)]\n"
     ]
    }
   ],
   "source": [
    "#Taking terms and Analyze term frequency by forming a dictionary of terms and their counts in the document-term matrix\n",
    "\n",
    "terms = np.genfromtxt(\"modifiedterms.txt\", dtype=str)\n",
    "terms[0:30]\n",
    "\n",
    "tFreq = DT_train.sum(axis=1)\n",
    "\n",
    "dictTF = {}\n",
    "for i in range(len(tFreq)):\n",
    "    dictTF[terms[i]] = tFreq[i]\n",
    "print(sorted(dictTF.items()))\n",
    "sorteddictTF = sorted(dictTF.values(), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWeJAMDMzAP4/+C4pSQhyDT8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a5b0e21dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sorteddictTF)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a. Create your own KNN classifier. Your classifier should allow as input the training data matrix, the training labels, the instance to be classified, the value of K, and should return the predicted class for the instance and the top K neighbors. Your classifier should work with Euclidean distance as well as Cosine Similarity (see class examples). You may create two separate classifiers, or add this capability as a parameter for the classifier function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DT_train = np.array(DT_train)\n",
    "DT_test = np.array(DT_test)\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def knn_search(x, D, L, K, measure):\n",
    "    \"\"\" find K nearest neighbours of data among D \"\"\"\n",
    "    if measure == 0:\n",
    "        # euclidean distances from the other points\n",
    "        inX = x\n",
    "        diffMat = np.tile(inX, (D.shape[0],1)) - D\n",
    "        sqDiffMat = diffMat**2\n",
    "        sqDistances = sqDiffMat.sum(axis=1)\n",
    "        dists = sqDistances**0.5\n",
    "    elif measure == 1:\n",
    "        D_norm = np.array([np.linalg.norm(D[i]) for i in range(len(D))])\n",
    "        x_norm = np.linalg.norm(x)\n",
    "        sims = np.dot(D,x)/(D_norm * x_norm)\n",
    "        dists = 1 - sims\n",
    "    #print(dists)\n",
    "    idx = np.argsort(dists) # sorting\n",
    "    #print(idx[:K])\n",
    "    # return the indexes of K nearest neighbours\n",
    "    neigh_labels = L[idx[:K]]\n",
    "    class0 = 0\n",
    "    class1 = 0\n",
    "    for index in neigh_labels:\n",
    "        #print(L[index])\n",
    "        if L[index] == 0:\n",
    "            class0 = class0 + 1\n",
    "        else:\n",
    "            class1 = class1 + 1\n",
    "        if class0 > class1:\n",
    "            predicted_class = 0\n",
    "        else:\n",
    "            predicted_class = 1\n",
    "    #print(\"The \" + str(K) + \" Nearest Neighbours are:\" + str(idx[:K]))\n",
    "    #print(\"The predicted class is: \" + str(predicted_class))\n",
    "    return idx[:K], predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just thought of printing it as its same as returning but didn't wanted to lose any grades for printing and not returning the values as the problem said it should return, so commented printing and just returning values.\n",
    "\n",
    "Trying a few vectors from the test data and the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Nearest Neighbours are:[685 628 667 152 427]\n",
      "The predicted class is: 1\n"
     ]
    }
   ],
   "source": [
    "neigh_idx, predicted_class = knn_search(DT_test[0], DT_train, train_labels, 5, 1)\n",
    "print(\"The Nearest Neighbours are:\" + str(neigh_idx))\n",
    "print(\"The predicted class is: \" + str(predicted_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Nearest Neighbours are:[685 628 703 266 510]\n",
      "The predicted class is: 1\n"
     ]
    }
   ],
   "source": [
    "neigh_idx, predicted_class = knn_search(DT_test[0], DT_train, train_labels, 5, 0)\n",
    "print(\"The Nearest Neighbours are:\" + str(neigh_idx))\n",
    "print(\"The predicted class is: \" + str(predicted_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Nearest Neighbours are:[382 775 163 587 550 127 485 651 767]\n",
      "The predicted class is: 0\n"
     ]
    }
   ],
   "source": [
    "neigh_idx, predicted_class = knn_search(DT_test[1], DT_train, train_labels, 9, 1)\n",
    "print(\"The Nearest Neighbours are:\" + str(neigh_idx))\n",
    "print(\"The predicted class is: \" + str(predicted_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Nearest Neighbours are:[798 554 757 224  38 119 711 398 608]\n",
      "The predicted class is: 0\n"
     ]
    }
   ],
   "source": [
    "neigh_idx, predicted_class = knn_search(DT_test[1], DT_train, train_labels, 9, 0)\n",
    "print(\"The Nearest Neighbours are:\" + str(neigh_idx))\n",
    "print(\"The predicted class is: \" + str(predicted_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Nearest Neighbours are:[255 405 556 744 170]\n",
      "The predicted class is: 1\n"
     ]
    }
   ],
   "source": [
    "neigh_idx, predicted_class = knn_search(DT_test[99], DT_train, train_labels, 5, 1)\n",
    "print(\"The Nearest Neighbours are:\" + str(neigh_idx))\n",
    "print(\"The predicted class is: \" + str(predicted_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Nearest Neighbours are:[744 405 556 170  49]\n",
      "The predicted class is: 1\n"
     ]
    }
   ],
   "source": [
    "neigh_idx, predicted_class = knn_search(DT_test[99], DT_train, train_labels, 5, 0)\n",
    "print(\"The Nearest Neighbours are:\" + str(neigh_idx))\n",
    "print(\"The predicted class is: \" + str(predicted_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Nearest Neighbours are:[322 664  14 145 440 792 232 254 546  52   5 246 203 618  73]\n",
      "The predicted class is: 1\n"
     ]
    }
   ],
   "source": [
    "neigh_idx, predicted_class = knn_search(DT_test[199], DT_train, train_labels, 15, 1)\n",
    "print(\"The Nearest Neighbours are:\" + str(neigh_idx))\n",
    "print(\"The predicted class is: \" + str(predicted_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Nearest Neighbours are:[798 757 224  38 119 711 398  69 751 342  53 644 706 551 464]\n",
      "The predicted class is: 0\n"
     ]
    }
   ],
   "source": [
    "neigh_idx, predicted_class = knn_search(DT_test[199], DT_train, train_labels, 15, 0)\n",
    "print(\"The Nearest Neighbours are:\" + str(neigh_idx))\n",
    "print(\"The predicted class is: \" + str(predicted_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So looking at the results above in one case which is the last one we can see different predictions for Euclidean and Cosine distance/similarity measures. If we look at the test labels here Cosine similarity (measure=1) results into correct prediction for that particular test vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b. Create a function to compute the classification accuracy over the test data set (ratio of correct predictions to the number of test instances). This function will call the classifier function on all the test instances and in each case compares the actual test class label to the predicted class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def accuracy(x, D, train_L, test_L, K, measure):\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    for i in range(x.shape[0]):\n",
    "        #Use the first i rows of the testing data, one-by-one, in the knn function\n",
    "        count += 1\n",
    "        neigh_idx, predicted_class = knn_search(x[i,:], D, train_L, K, measure)\n",
    "        #If the two classes don't match, add 1 to the error count\n",
    "        if (predicted_class == test_L[i]):\n",
    "            correct += 1\n",
    "    accuracypercent = (correct/count)*100\n",
    "    print(\"Accuracy is: \" + str(accuracypercent) + \"%\")\n",
    "    return accuracypercent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n",
      "200\n",
      "Accuracy is: 81.5%\n"
     ]
    }
   ],
   "source": [
    "accuracypercent = accuracy(DT_test, DT_train, train_labels, test_labels, 5, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c. Run your accuracy function on a range of values for K in order to compare accuracy values for different numbers of neighbors. Do this both using Euclidean Distance as well as Cosine similarity measure. [For example, you can try evaluating your classifiers on a range of values of K from 1 through 20 and present the results as a table or a graph]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "def plot_accuracy(x, D, train_L, test_L):\n",
    "    i = 1\n",
    "    accEuc = {}\n",
    "    accCos = {}\n",
    "    for i in range(20):\n",
    "        i += 1\n",
    "        accEuc[i] = accuracy(x, D, train_L, test_L, i, 0)\n",
    "        #print(accEuc)\n",
    "        accCos[i] = accuracy(x, D, train_L, test_L, i, 1)\n",
    "        #print(accCos)\n",
    "    df_toPlotEuc = pd.DataFrame(list(accEuc.items()))\n",
    "    df_toPlotCos = pd.DataFrame(list(accCos.items()))\n",
    "    plt.plot( df_toPlotEuc[0], df_toPlotEuc[1], 'r^--', label=\"Eucliudean\")\n",
    "    plt.plot( df_toPlotCos[0], df_toPlotCos[1], 'bs-', label=\"Cosine\")\n",
    "    plt.xlabel(\"Number of Neighbours\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy Comparison For k-Nearest Neighbours\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 78.0%\n",
      "{1: 78.0}\n",
      "Accuracy is: 98.5%\n",
      "{1: 98.5}\n",
      "Accuracy is: 67.5%\n",
      "{1: 78.0, 2: 67.5}\n",
      "Accuracy is: 98.0%\n",
      "{1: 98.5, 2: 98.0}\n",
      "Accuracy is: 81.0%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0}\n",
      "Accuracy is: 97.0%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0}\n",
      "Accuracy is: 77.0%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0}\n",
      "Accuracy is: 98.0%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0}\n",
      "Accuracy is: 81.5%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5}\n",
      "Accuracy is: 97.0%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0}\n",
      "Accuracy is: 83.5%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5}\n",
      "Accuracy is: 97.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5}\n",
      "Accuracy is: 76.5%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5}\n",
      "Accuracy is: 98.0%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0}\n",
      "Accuracy is: 80.0%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0}\n",
      "Accuracy is: 97.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5}\n",
      "Accuracy is: 75.0%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0}\n",
      "Accuracy is: 97.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5}\n",
      "Accuracy is: 86.0%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0}\n",
      "Accuracy is: 98.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5}\n",
      "Accuracy is: 79.5%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0, 11: 79.5}\n",
      "Accuracy is: 98.0%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5, 11: 98.0}\n",
      "Accuracy is: 85.0%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0, 11: 79.5, 12: 85.0}\n",
      "Accuracy is: 98.0%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5, 11: 98.0, 12: 98.0}\n",
      "Accuracy is: 77.5%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0, 11: 79.5, 12: 85.0, 13: 77.5}\n",
      "Accuracy is: 98.0%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5, 11: 98.0, 12: 98.0, 13: 98.0}\n",
      "Accuracy is: 82.0%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0, 11: 79.5, 12: 85.0, 13: 77.5, 14: 82.0}\n",
      "Accuracy is: 98.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5, 11: 98.0, 12: 98.0, 13: 98.0, 14: 98.5}\n",
      "Accuracy is: 78.5%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0, 11: 79.5, 12: 85.0, 13: 77.5, 14: 82.0, 15: 78.5}\n",
      "Accuracy is: 98.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5, 11: 98.0, 12: 98.0, 13: 98.0, 14: 98.5, 15: 98.5}\n",
      "Accuracy is: 80.0%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0, 11: 79.5, 12: 85.0, 13: 77.5, 14: 82.0, 15: 78.5, 16: 80.0}\n",
      "Accuracy is: 98.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5, 11: 98.0, 12: 98.0, 13: 98.0, 14: 98.5, 15: 98.5, 16: 98.5}\n",
      "Accuracy is: 76.0%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0, 11: 79.5, 12: 85.0, 13: 77.5, 14: 82.0, 15: 78.5, 16: 80.0, 17: 76.0}\n",
      "Accuracy is: 97.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5, 11: 98.0, 12: 98.0, 13: 98.0, 14: 98.5, 15: 98.5, 16: 98.5, 17: 97.5}\n",
      "Accuracy is: 78.5%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0, 11: 79.5, 12: 85.0, 13: 77.5, 14: 82.0, 15: 78.5, 16: 80.0, 17: 76.0, 18: 78.5}\n",
      "Accuracy is: 97.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5, 11: 98.0, 12: 98.0, 13: 98.0, 14: 98.5, 15: 98.5, 16: 98.5, 17: 97.5, 18: 97.5}\n",
      "Accuracy is: 74.0%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0, 11: 79.5, 12: 85.0, 13: 77.5, 14: 82.0, 15: 78.5, 16: 80.0, 17: 76.0, 18: 78.5, 19: 74.0}\n",
      "Accuracy is: 97.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5, 11: 98.0, 12: 98.0, 13: 98.0, 14: 98.5, 15: 98.5, 16: 98.5, 17: 97.5, 18: 97.5, 19: 97.5}\n",
      "Accuracy is: 76.5%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0, 11: 79.5, 12: 85.0, 13: 77.5, 14: 82.0, 15: 78.5, 16: 80.0, 17: 76.0, 18: 78.5, 19: 74.0, 20: 76.5}\n",
      "Accuracy is: 97.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5, 11: 98.0, 12: 98.0, 13: 98.0, 14: 98.5, 15: 98.5, 16: 98.5, 17: 97.5, 18: 97.5, 19: 97.5, 20: 97.5}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEZCAYAAABiu9n+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a5b0ea2da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_accuracy(DT_test, DT_train, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the graph above we can see that the performance of the classifier is much better with Cosine similarity as similarity measure as compared to when we use Euclidean distance. \n",
    "\n",
    "The maximum accuracy achieved using Cosine similarity is 98.5% which is for k = 1, 10, 14, 15, 16.\n",
    "\n",
    "The maximum accuracy achieved using Euclidean distance is 86% which is for k = 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d. Using Python, modify the training and test data sets so that term weights are converted to TFxIDF weights (instead of raw term frequencies). [See class notes on Text Categorization]. Then, rerun your evaluation on the range of K values (as above) and compare the results to the results without using TFxIDF weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5500\n",
      "800\n",
      "5500\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "#Loading term-document data\n",
    "train_data = pd.read_table('trainMatrixModified.txt',header=None)\n",
    "test_data = pd.read_table('testMatrixModified.txt',header=None)\n",
    "\n",
    "TD_train = np.array(train_data)\n",
    "TD_test = np.array(test_data)\n",
    "\n",
    "numTerms_train = len(TD_train[:,0])\n",
    "NDocs_train = len(TD_train[0])\n",
    "print(numTerms_train)\n",
    "print(NDocs_train)\n",
    "\n",
    "numTerms_test = len(TD_test[:,0])\n",
    "NDocs_test = len(TD_test[0])\n",
    "print(numTerms_test)\n",
    "print(NDocs_test)\n",
    "\n",
    "#Finding document frequencies\n",
    "DF_train = np.array([(TD_train!=0).sum(1)]).T\n",
    "DF_test = np.array([(TD_test!=0).sum(1)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50]\n",
      " [ 7]\n",
      " [10]\n",
      " ..., \n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]]\n"
     ]
    }
   ],
   "source": [
    "print(DF_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15]\n",
      " [ 0]\n",
      " [ 1]\n",
      " ..., \n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]]\n"
     ]
    }
   ],
   "source": [
    "print(DF_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 800.  800.  800. ...,  800.  800.  800.]\n",
      " [ 800.  800.  800. ...,  800.  800.  800.]\n",
      " [ 800.  800.  800. ...,  800.  800.  800.]\n",
      " ..., \n",
      " [ 800.  800.  800. ...,  800.  800.  800.]\n",
      " [ 800.  800.  800. ...,  800.  800.  800.]\n",
      " [ 800.  800.  800. ...,  800.  800.  800.]]\n",
      "[[ 200.  200.  200. ...,  200.  200.  200.]\n",
      " [ 200.  200.  200. ...,  200.  200.  200.]\n",
      " [ 200.  200.  200. ...,  200.  200.  200.]\n",
      " ..., \n",
      " [ 200.  200.  200. ...,  200.  200.  200.]\n",
      " [ 200.  200.  200. ...,  200.  200.  200.]\n",
      " [ 200.  200.  200. ...,  200.  200.  200.]]\n"
     ]
    }
   ],
   "source": [
    "#Create a matrix with all entries = NDocs\n",
    "NMatrix_train = np.ones(np.shape(TD_train), dtype=float)*NDocs_train\n",
    "print(NMatrix_train)\n",
    "\n",
    "NMatrix_test = np.ones(np.shape(TD_test), dtype=float)*NDocs_test\n",
    "print(NMatrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.          4.          4.         ...,  4.          4.          4.        ]\n",
      " [ 6.83650127  6.83650127  6.83650127 ...,  6.83650127  6.83650127\n",
      "   6.83650127]\n",
      " [ 6.32192809  6.32192809  6.32192809 ...,  6.32192809  6.32192809\n",
      "   6.32192809]\n",
      " ..., \n",
      " [ 8.64385619  8.64385619  8.64385619 ...,  8.64385619  8.64385619\n",
      "   8.64385619]\n",
      " [ 8.64385619  8.64385619  8.64385619 ...,  8.64385619  8.64385619\n",
      "   8.64385619]\n",
      " [ 8.64385619  8.64385619  8.64385619 ...,  8.64385619  8.64385619\n",
      "   8.64385619]]\n",
      "[[ 3.73696559  3.73696559  3.73696559 ...,  3.73696559  3.73696559\n",
      "   3.73696559]\n",
      " [        inf         inf         inf ...,         inf         inf\n",
      "          inf]\n",
      " [ 7.64385619  7.64385619  7.64385619 ...,  7.64385619  7.64385619\n",
      "   7.64385619]\n",
      " ..., \n",
      " [ 7.64385619  7.64385619  7.64385619 ...,  7.64385619  7.64385619\n",
      "   7.64385619]\n",
      " [ 7.64385619  7.64385619  7.64385619 ...,  7.64385619  7.64385619\n",
      "   7.64385619]\n",
      " [ 7.64385619  7.64385619  7.64385619 ...,  7.64385619  7.64385619\n",
      "   7.64385619]]\n"
     ]
    }
   ],
   "source": [
    "#Converting into IDF Values\n",
    "\n",
    "IDF_train = np.log2(np.divide(NMatrix_train, DF_train))\n",
    "print(IDF_train)\n",
    "\n",
    "IDF_test = np.log2(np.divide(NMatrix_test, DF_test))\n",
    "print(IDF_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8.           0.           0.         ...,   0.           0.           0.        ]\n",
      " [ 13.67300254   0.           0.         ...,   0.           0.           0.        ]\n",
      " [ 12.64385619   0.           0.         ...,   0.           0.           0.        ]\n",
      " ..., \n",
      " [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      " [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      " [  0.           0.           0.         ...,   0.           0.           0.        ]]\n",
      "[[ 0.          0.          3.73696559 ...,  0.          0.          0.        ]\n",
      " [        nan         nan         nan ...,         nan         nan\n",
      "          nan]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Computing TFxIDF values for each document-term entry\n",
    "\n",
    "TF_IDF_train = TD_train*IDF_train\n",
    "print(TF_IDF_train)\n",
    "\n",
    "TF_IDF_test = TD_test*IDF_test\n",
    "print(TF_IDF_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 78.0%\n",
      "{1: 78.0}\n",
      "Accuracy is: 98.5%\n",
      "{1: 98.5}\n",
      "Accuracy is: 67.5%\n",
      "{1: 78.0, 2: 67.5}\n",
      "Accuracy is: 98.0%\n",
      "{1: 98.5, 2: 98.0}\n",
      "Accuracy is: 81.0%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0}\n",
      "Accuracy is: 97.0%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0}\n",
      "Accuracy is: 77.0%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0}\n",
      "Accuracy is: 98.0%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0}\n",
      "Accuracy is: 81.5%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5}\n",
      "Accuracy is: 97.0%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0}\n",
      "Accuracy is: 83.5%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5}\n",
      "Accuracy is: 97.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5}\n",
      "Accuracy is: 76.5%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5}\n",
      "Accuracy is: 98.0%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0}\n",
      "Accuracy is: 80.0%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0}\n",
      "Accuracy is: 97.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5}\n",
      "Accuracy is: 75.0%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0}\n",
      "Accuracy is: 97.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5}\n",
      "Accuracy is: 86.0%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0}\n",
      "Accuracy is: 98.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5}\n",
      "Accuracy is: 79.5%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0, 11: 79.5}\n",
      "Accuracy is: 98.0%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5, 11: 98.0}\n",
      "Accuracy is: 85.0%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0, 11: 79.5, 12: 85.0}\n",
      "Accuracy is: 98.0%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5, 11: 98.0, 12: 98.0}\n",
      "Accuracy is: 77.5%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0, 11: 79.5, 12: 85.0, 13: 77.5}\n",
      "Accuracy is: 98.0%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5, 11: 98.0, 12: 98.0, 13: 98.0}\n",
      "Accuracy is: 82.0%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0, 11: 79.5, 12: 85.0, 13: 77.5, 14: 82.0}\n",
      "Accuracy is: 98.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5, 11: 98.0, 12: 98.0, 13: 98.0, 14: 98.5}\n",
      "Accuracy is: 78.5%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0, 11: 79.5, 12: 85.0, 13: 77.5, 14: 82.0, 15: 78.5}\n",
      "Accuracy is: 98.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5, 11: 98.0, 12: 98.0, 13: 98.0, 14: 98.5, 15: 98.5}\n",
      "Accuracy is: 80.0%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0, 11: 79.5, 12: 85.0, 13: 77.5, 14: 82.0, 15: 78.5, 16: 80.0}\n",
      "Accuracy is: 98.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5, 11: 98.0, 12: 98.0, 13: 98.0, 14: 98.5, 15: 98.5, 16: 98.5}\n",
      "Accuracy is: 76.0%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0, 11: 79.5, 12: 85.0, 13: 77.5, 14: 82.0, 15: 78.5, 16: 80.0, 17: 76.0}\n",
      "Accuracy is: 97.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5, 11: 98.0, 12: 98.0, 13: 98.0, 14: 98.5, 15: 98.5, 16: 98.5, 17: 97.5}\n",
      "Accuracy is: 78.5%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0, 11: 79.5, 12: 85.0, 13: 77.5, 14: 82.0, 15: 78.5, 16: 80.0, 17: 76.0, 18: 78.5}\n",
      "Accuracy is: 97.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5, 11: 98.0, 12: 98.0, 13: 98.0, 14: 98.5, 15: 98.5, 16: 98.5, 17: 97.5, 18: 97.5}\n",
      "Accuracy is: 74.0%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0, 11: 79.5, 12: 85.0, 13: 77.5, 14: 82.0, 15: 78.5, 16: 80.0, 17: 76.0, 18: 78.5, 19: 74.0}\n",
      "Accuracy is: 97.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5, 11: 98.0, 12: 98.0, 13: 98.0, 14: 98.5, 15: 98.5, 16: 98.5, 17: 97.5, 18: 97.5, 19: 97.5}\n",
      "Accuracy is: 76.5%\n",
      "{1: 78.0, 2: 67.5, 3: 81.0, 4: 77.0, 5: 81.5, 6: 83.5, 7: 76.5, 8: 80.0, 9: 75.0, 10: 86.0, 11: 79.5, 12: 85.0, 13: 77.5, 14: 82.0, 15: 78.5, 16: 80.0, 17: 76.0, 18: 78.5, 19: 74.0, 20: 76.5}\n",
      "Accuracy is: 97.5%\n",
      "{1: 98.5, 2: 98.0, 3: 97.0, 4: 98.0, 5: 97.0, 6: 97.5, 7: 98.0, 8: 97.5, 9: 97.5, 10: 98.5, 11: 98.0, 12: 98.0, 13: 98.0, 14: 98.5, 15: 98.5, 16: 98.5, 17: 97.5, 18: 97.5, 19: 97.5, 20: 97.5}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEZCAYAAABiu9n+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a5b1b7a048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_accuracy(TF_IDF_test, TF_IDF_train, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e. Create a classifier based on the Rocchio Method adapted for text categorization. Your classifier should take as input the training data matrix, the training labels, and the instance to be classified. It should compute the prototype vectors for each of the categories and measure Cosine similarity of the test instance to each prototype. Your output should indicate the predicted class and include the similarity values of the instance to each of the category prototypes. Finally, compute the classification accuracy using the test instances and compare your results to the best KNN approach you tried earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification using scikit-learn [Dataset: bank_data.csv]\n",
    "For this problem you will experiment with various classifiers provided as part of the scikit-learn (sklearn) machine learning module, as well as with some of its preprocessing and model evaluation capabilities.  [Note: This module is already part of the Anaconda distributions. However, if you are using standalone Python distributions, you will need to first obtain and install it]. You will work with a modified subset of a real data set of customers for a bank. This is the same data set used in Assignment 1. The data is provided in a CSV formatted file with the first row containing the attribute names. The description of the the different fields in the data are provided in this document.\n",
    "\n",
    "Your tasks in this problem are the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a. Load and preprocess the data using Numpy or Pandas and the preprocessing functions from scikit-learn. Specifically, you need to separate the target attribute (\"pep\") from the portion of the data to be used for training and testing. You will need to convert the selected dataset into the Standard Spreadsheet format (scikit-learn functions generally assume that all attributes are in numeric form). Finally, you need to split the transformed data into training and test sets (using 80%-20% randomized split). [Review Ipython Notebook examples from Week 4 for different ways to perform these tasks.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\GunjanPandya\\\\Downloads\\\\CSC478\\\\newsgroups'"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GunjanPandya\\Downloads\\CSC478\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\GunjanPandya\\Downloads\\CSC478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_bank_data = pd.read_csv('bank_data.csv',index_col=0)\n",
    "\n",
    "#We will not need ID anyways for classification, so rather then dropping it before converting it to CSV, storing it as IndexError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>children</th>\n",
       "      <th>gender</th>\n",
       "      <th>region</th>\n",
       "      <th>married</th>\n",
       "      <th>car</th>\n",
       "      <th>savings_acct</th>\n",
       "      <th>current_acct</th>\n",
       "      <th>mortgage</th>\n",
       "      <th>pep</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID12101</th>\n",
       "      <td>48</td>\n",
       "      <td>17546.00</td>\n",
       "      <td>1</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>INNER_CITY</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12102</th>\n",
       "      <td>40</td>\n",
       "      <td>30085.10</td>\n",
       "      <td>3</td>\n",
       "      <td>MALE</td>\n",
       "      <td>TOWN</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12103</th>\n",
       "      <td>51</td>\n",
       "      <td>16575.40</td>\n",
       "      <td>0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>INNER_CITY</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12104</th>\n",
       "      <td>23</td>\n",
       "      <td>20375.40</td>\n",
       "      <td>3</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>TOWN</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12105</th>\n",
       "      <td>57</td>\n",
       "      <td>50576.30</td>\n",
       "      <td>0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>RURAL</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12106</th>\n",
       "      <td>57</td>\n",
       "      <td>37869.60</td>\n",
       "      <td>2</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>TOWN</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12107</th>\n",
       "      <td>22</td>\n",
       "      <td>8877.07</td>\n",
       "      <td>0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>RURAL</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12108</th>\n",
       "      <td>58</td>\n",
       "      <td>24946.60</td>\n",
       "      <td>0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>TOWN</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12109</th>\n",
       "      <td>37</td>\n",
       "      <td>25304.30</td>\n",
       "      <td>2</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>SUBURBAN</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12110</th>\n",
       "      <td>54</td>\n",
       "      <td>24212.10</td>\n",
       "      <td>2</td>\n",
       "      <td>MALE</td>\n",
       "      <td>TOWN</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         age    income  children  gender      region married  car  \\\n",
       "id                                                                  \n",
       "ID12101   48  17546.00         1  FEMALE  INNER_CITY      NO   NO   \n",
       "ID12102   40  30085.10         3    MALE        TOWN     YES  YES   \n",
       "ID12103   51  16575.40         0  FEMALE  INNER_CITY     YES  YES   \n",
       "ID12104   23  20375.40         3  FEMALE        TOWN     YES   NO   \n",
       "ID12105   57  50576.30         0  FEMALE       RURAL     YES   NO   \n",
       "ID12106   57  37869.60         2  FEMALE        TOWN     YES   NO   \n",
       "ID12107   22   8877.07         0    MALE       RURAL      NO   NO   \n",
       "ID12108   58  24946.60         0    MALE        TOWN     YES  YES   \n",
       "ID12109   37  25304.30         2  FEMALE    SUBURBAN     YES  YES   \n",
       "ID12110   54  24212.10         2    MALE        TOWN     YES  YES   \n",
       "\n",
       "        savings_acct current_acct mortgage  pep  \n",
       "id                                               \n",
       "ID12101           NO           NO       NO  YES  \n",
       "ID12102           NO          YES      YES   NO  \n",
       "ID12103          YES          YES       NO   NO  \n",
       "ID12104           NO          YES       NO   NO  \n",
       "ID12105          YES           NO       NO   NO  \n",
       "ID12106          YES          YES       NO  YES  \n",
       "ID12107           NO          YES       NO  YES  \n",
       "ID12108          YES          YES       NO   NO  \n",
       "ID12109           NO           NO       NO   NO  \n",
       "ID12110          YES          YES       NO   NO  "
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bank_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 11)"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bank_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's separate the target attribute and the attributes used for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>children</th>\n",
       "      <th>gender</th>\n",
       "      <th>region</th>\n",
       "      <th>married</th>\n",
       "      <th>car</th>\n",
       "      <th>savings_acct</th>\n",
       "      <th>current_acct</th>\n",
       "      <th>mortgage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID12101</th>\n",
       "      <td>48</td>\n",
       "      <td>17546.0</td>\n",
       "      <td>1</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>INNER_CITY</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12102</th>\n",
       "      <td>40</td>\n",
       "      <td>30085.1</td>\n",
       "      <td>3</td>\n",
       "      <td>MALE</td>\n",
       "      <td>TOWN</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12103</th>\n",
       "      <td>51</td>\n",
       "      <td>16575.4</td>\n",
       "      <td>0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>INNER_CITY</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12104</th>\n",
       "      <td>23</td>\n",
       "      <td>20375.4</td>\n",
       "      <td>3</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>TOWN</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12105</th>\n",
       "      <td>57</td>\n",
       "      <td>50576.3</td>\n",
       "      <td>0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>RURAL</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         age   income  children  gender      region married  car savings_acct  \\\n",
       "id                                                                              \n",
       "ID12101   48  17546.0         1  FEMALE  INNER_CITY      NO   NO           NO   \n",
       "ID12102   40  30085.1         3    MALE        TOWN     YES  YES           NO   \n",
       "ID12103   51  16575.4         0  FEMALE  INNER_CITY     YES  YES          YES   \n",
       "ID12104   23  20375.4         3  FEMALE        TOWN     YES   NO           NO   \n",
       "ID12105   57  50576.3         0  FEMALE       RURAL     YES   NO          YES   \n",
       "\n",
       "        current_acct mortgage  \n",
       "id                             \n",
       "ID12101           NO       NO  \n",
       "ID12102          YES      YES  \n",
       "ID12103          YES       NO  \n",
       "ID12104          YES       NO  \n",
       "ID12105           NO       NO  "
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_records = df_bank_data[['age','income','children','gender','region','married','car','savings_acct','current_acct','mortgage']]\n",
    "df_records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    YES\n",
       "1     NO\n",
       "2     NO\n",
       "3     NO\n",
       "4     NO\n",
       "Name: pep, dtype: object"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target = df_bank_data.pep\n",
    "df_target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use Pandas \"get_dummies\" function to create dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_ssf = pd.get_dummies(df_records, columns = [\"gender\",\"region\",\"married\",\"car\",\"savings_acct\",\"current_acct\",\"mortgage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>children</th>\n",
       "      <th>gender_FEMALE</th>\n",
       "      <th>gender_MALE</th>\n",
       "      <th>region_INNER_CITY</th>\n",
       "      <th>region_RURAL</th>\n",
       "      <th>region_SUBURBAN</th>\n",
       "      <th>region_TOWN</th>\n",
       "      <th>married_NO</th>\n",
       "      <th>married_YES</th>\n",
       "      <th>car_NO</th>\n",
       "      <th>car_YES</th>\n",
       "      <th>savings_acct_NO</th>\n",
       "      <th>savings_acct_YES</th>\n",
       "      <th>current_acct_NO</th>\n",
       "      <th>current_acct_YES</th>\n",
       "      <th>mortgage_NO</th>\n",
       "      <th>mortgage_YES</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID12101</th>\n",
       "      <td>48</td>\n",
       "      <td>17546.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12102</th>\n",
       "      <td>40</td>\n",
       "      <td>30085.10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12103</th>\n",
       "      <td>51</td>\n",
       "      <td>16575.40</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12104</th>\n",
       "      <td>23</td>\n",
       "      <td>20375.40</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12105</th>\n",
       "      <td>57</td>\n",
       "      <td>50576.30</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12106</th>\n",
       "      <td>57</td>\n",
       "      <td>37869.60</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12107</th>\n",
       "      <td>22</td>\n",
       "      <td>8877.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12108</th>\n",
       "      <td>58</td>\n",
       "      <td>24946.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12109</th>\n",
       "      <td>37</td>\n",
       "      <td>25304.30</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID12110</th>\n",
       "      <td>54</td>\n",
       "      <td>24212.10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         age    income  children  gender_FEMALE  gender_MALE  \\\n",
       "id                                                             \n",
       "ID12101   48  17546.00         1            1.0          0.0   \n",
       "ID12102   40  30085.10         3            0.0          1.0   \n",
       "ID12103   51  16575.40         0            1.0          0.0   \n",
       "ID12104   23  20375.40         3            1.0          0.0   \n",
       "ID12105   57  50576.30         0            1.0          0.0   \n",
       "ID12106   57  37869.60         2            1.0          0.0   \n",
       "ID12107   22   8877.07         0            0.0          1.0   \n",
       "ID12108   58  24946.60         0            0.0          1.0   \n",
       "ID12109   37  25304.30         2            1.0          0.0   \n",
       "ID12110   54  24212.10         2            0.0          1.0   \n",
       "\n",
       "         region_INNER_CITY  region_RURAL  region_SUBURBAN  region_TOWN  \\\n",
       "id                                                                       \n",
       "ID12101                1.0           0.0              0.0          0.0   \n",
       "ID12102                0.0           0.0              0.0          1.0   \n",
       "ID12103                1.0           0.0              0.0          0.0   \n",
       "ID12104                0.0           0.0              0.0          1.0   \n",
       "ID12105                0.0           1.0              0.0          0.0   \n",
       "ID12106                0.0           0.0              0.0          1.0   \n",
       "ID12107                0.0           1.0              0.0          0.0   \n",
       "ID12108                0.0           0.0              0.0          1.0   \n",
       "ID12109                0.0           0.0              1.0          0.0   \n",
       "ID12110                0.0           0.0              0.0          1.0   \n",
       "\n",
       "         married_NO  married_YES  car_NO  car_YES  savings_acct_NO  \\\n",
       "id                                                                   \n",
       "ID12101         1.0          0.0     1.0      0.0              1.0   \n",
       "ID12102         0.0          1.0     0.0      1.0              1.0   \n",
       "ID12103         0.0          1.0     0.0      1.0              0.0   \n",
       "ID12104         0.0          1.0     1.0      0.0              1.0   \n",
       "ID12105         0.0          1.0     1.0      0.0              0.0   \n",
       "ID12106         0.0          1.0     1.0      0.0              0.0   \n",
       "ID12107         1.0          0.0     1.0      0.0              1.0   \n",
       "ID12108         0.0          1.0     0.0      1.0              0.0   \n",
       "ID12109         0.0          1.0     0.0      1.0              1.0   \n",
       "ID12110         0.0          1.0     0.0      1.0              0.0   \n",
       "\n",
       "         savings_acct_YES  current_acct_NO  current_acct_YES  mortgage_NO  \\\n",
       "id                                                                          \n",
       "ID12101               0.0              1.0               0.0          1.0   \n",
       "ID12102               0.0              0.0               1.0          0.0   \n",
       "ID12103               1.0              0.0               1.0          1.0   \n",
       "ID12104               0.0              0.0               1.0          1.0   \n",
       "ID12105               1.0              1.0               0.0          1.0   \n",
       "ID12106               1.0              0.0               1.0          1.0   \n",
       "ID12107               0.0              0.0               1.0          1.0   \n",
       "ID12108               1.0              0.0               1.0          1.0   \n",
       "ID12109               0.0              1.0               0.0          1.0   \n",
       "ID12110               1.0              0.0               1.0          1.0   \n",
       "\n",
       "         mortgage_YES  \n",
       "id                     \n",
       "ID12101           0.0  \n",
       "ID12102           1.0  \n",
       "ID12103           0.0  \n",
       "ID12104           0.0  \n",
       "ID12105           0.0  \n",
       "ID12106           0.0  \n",
       "ID12107           0.0  \n",
       "ID12108           0.0  \n",
       "ID12109           0.0  \n",
       "ID12110           0.0  "
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ssf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we divide the data into randomized training and test partitions (note that the same split should also be perfromed on the target attribute). The easiest way to do this is to use the \"train_test_split\" module of \"sklearn.cross_validation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "df_train, df_test, df_target_train, df_target_test = train_test_split(data_ssf, df_target, test_size=0.2, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 19)"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 19)"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480,)"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b. Run scikit-learn's KNN classifier on the test set. Note: in the case of KNN, you must first normalize the data so that all attributes are in the same scale (normalize so that the values are between 0 and 1). Generate the confusion matrix (visualize it using Matplotlib), as well as the classification report. Also, computing the average accuracy score. Experiment with different values of K and the weight parameter for KNN to see if you can improve accuracy (you do not need to provide the details of all of your experimentation, but provide a short discussion what parameters worked best)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the data to be used in the KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler().fit(df_train)\n",
    "df_train_norm = min_max_scaler.transform(df_train)\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler().fit(df_test)\n",
    "df_test_norm = min_max_scaler.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.57,  0.46,  0.67,  1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "         1.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  1.  ,  1.  ,  0.  ],\n",
       "       [ 0.8 ,  0.78,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,\n",
       "         1.  ,  1.  ,  0.  ,  0.  ,  1.  ,  1.  ,  0.  ,  1.  ,  0.  ],\n",
       "       [ 0.14,  0.16,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "         1.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  1.  ,  0.  ],\n",
       "       [ 0.35,  0.18,  0.33,  0.  ,  1.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "         1.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  1.  ,  0.  ],\n",
       "       [ 0.33,  0.18,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  ,\n",
       "         1.  ,  0.  ,  1.  ,  0.  ,  1.  ,  1.  ,  0.  ,  0.  ,  1.  ]])"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=2, linewidth=80, suppress=True)\n",
    "df_train_norm[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.29,  0.15,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  ,\n",
       "         1.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  1.  ],\n",
       "       [ 0.84,  0.45,  0.33,  1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  1.  ,\n",
       "         0.  ,  0.  ,  1.  ,  1.  ,  0.  ,  0.  ,  1.  ,  1.  ,  0.  ],\n",
       "       [ 0.88,  0.67,  1.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,\n",
       "         1.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  1.  ],\n",
       "       [ 0.71,  0.8 ,  1.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,\n",
       "         1.  ,  0.  ,  1.  ,  0.  ,  1.  ,  1.  ,  0.  ,  1.  ,  0.  ],\n",
       "       [ 0.96,  0.6 ,  0.33,  0.  ,  1.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "         1.  ,  0.  ,  1.  ,  0.  ,  1.  ,  1.  ,  0.  ,  0.  ,  1.  ]])"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_norm[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=4, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#k-nearest neighbors classifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "bank_data_knn = KNeighborsClassifier(n_neighbors = 4, metric='euclidean')\n",
    "bank_data_knn.fit(df_train_norm, df_target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['YES', 'YES', 'NO', 'NO', 'YES', 'YES', 'YES', 'NO', 'NO', 'YES', 'NO',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'YES', 'NO', 'YES', 'NO', 'NO', 'YES',\n",
       "       'NO', 'NO', 'YES', 'NO', 'YES', 'YES', 'NO', 'YES', 'NO', 'NO', 'NO',\n",
       "       'NO', 'NO', 'YES', 'NO', 'YES', 'NO', 'NO', 'NO', 'NO', 'YES', 'NO',\n",
       "       'YES', 'NO', 'YES', 'YES', 'NO', 'YES', 'YES', 'YES', 'YES', 'NO', 'YES',\n",
       "       'YES', 'YES', 'NO', 'NO', 'NO', 'NO', 'NO', 'YES', 'YES', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', 'YES', 'YES', 'NO', 'YES', 'NO', 'NO', 'NO', 'NO',\n",
       "       'YES', 'YES', 'NO', 'NO', 'YES', 'NO', 'YES', 'NO', 'YES', 'YES', 'YES',\n",
       "       'NO', 'YES', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'YES', 'NO', 'YES', 'NO',\n",
       "       'NO', 'YES', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO'], dtype=object)"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target_predict = bank_data_knn.predict(df_test_norm)\n",
    "df_target_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         NO       0.65      0.77      0.71        66\n",
      "        YES       0.64      0.50      0.56        54\n",
      "\n",
      "avg / total       0.65      0.65      0.64       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(df_target_test, df_target_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix - And its visualization using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51 15]\n",
      " [27 27]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAADvCAYAAADy1WG7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a5b10ac860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "knn_confusion_matrix = confusion_matrix(y_true = df_target_test, y_pred = df_target_predict)\n",
    "\n",
    "print(knn_confusion_matrix)\n",
    "\n",
    "plt.matshow(knn_confusion_matrix)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Accuracy Score (over test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65000000000000002"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_data_knn.score(df_test_norm, df_target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Accuracy Score (over training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75624999999999998"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_data_knn.score(df_train_norm, df_target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         NO       0.67      0.71      0.69        66\n",
      "        YES       0.62      0.57      0.60        54\n",
      "\n",
      "avg / total       0.65      0.65      0.65       120\n",
      "\n",
      "[[47 19]\n",
      " [23 31]]\n",
      "Accuracy on test data:0.65\n",
      "Accuracy on training data:1.0\n"
     ]
    }
   ],
   "source": [
    "#Changing the kNN parameters (k, metric, weights)\n",
    "\n",
    "bank_data_knn = KNeighborsClassifier(n_neighbors = 5, weights = 'distance', metric='euclidean')\n",
    "bank_data_knn.fit(df_train_norm, df_target_train)\n",
    "df_target_predict = bank_data_knn.predict(df_test_norm)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_target_test, df_target_predict))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "knn_confusion_matrix = confusion_matrix(y_true = df_target_test, y_pred = df_target_predict)\n",
    "print(knn_confusion_matrix)\n",
    "\n",
    "print(\"Accuracy on test data:\" + str(bank_data_knn.score(df_test_norm, df_target_test)))\n",
    "print(\"Accuracy on training data:\" + str(bank_data_knn.score(df_train_norm, df_target_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         NO       0.67      0.73      0.70        66\n",
      "        YES       0.62      0.56      0.59        54\n",
      "\n",
      "avg / total       0.65      0.65      0.65       120\n",
      "\n",
      "[[48 18]\n",
      " [24 30]]\n",
      "Accuracy on test data:0.65\n",
      "Accuracy on training data:1.0\n"
     ]
    }
   ],
   "source": [
    "bank_data_knn = KNeighborsClassifier(n_neighbors = 7, weights = 'distance', metric='euclidean')\n",
    "bank_data_knn.fit(df_train_norm, df_target_train)\n",
    "df_target_predict = bank_data_knn.predict(df_test_norm)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_target_test, df_target_predict))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "knn_confusion_matrix = confusion_matrix(y_true = df_target_test, y_pred = df_target_predict)\n",
    "print(knn_confusion_matrix)\n",
    "\n",
    "print(\"Accuracy on test data:\" + str(bank_data_knn.score(df_test_norm, df_target_test)))\n",
    "print(\"Accuracy on training data:\" + str(bank_data_knn.score(df_train_norm, df_target_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         NO       0.68      0.70      0.69        66\n",
      "        YES       0.62      0.59      0.60        54\n",
      "\n",
      "avg / total       0.65      0.65      0.65       120\n",
      "\n",
      "[[46 20]\n",
      " [22 32]]\n",
      "Accuracy on test data:0.65\n",
      "Accuracy on training data:1.0\n"
     ]
    }
   ],
   "source": [
    "bank_data_knn = KNeighborsClassifier(n_neighbors = 9, weights = 'distance', metric='euclidean')\n",
    "bank_data_knn.fit(df_train_norm, df_target_train)\n",
    "df_target_predict = bank_data_knn.predict(df_test_norm)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_target_test, df_target_predict))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "knn_confusion_matrix = confusion_matrix(y_true = df_target_test, y_pred = df_target_predict)\n",
    "print(knn_confusion_matrix)\n",
    "\n",
    "print(\"Accuracy on test data:\" + str(bank_data_knn.score(df_test_norm, df_target_test)))\n",
    "print(\"Accuracy on training data:\" + str(bank_data_knn.score(df_train_norm, df_target_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         NO       0.68      0.71      0.70        66\n",
      "        YES       0.63      0.59      0.61        54\n",
      "\n",
      "avg / total       0.66      0.66      0.66       120\n",
      "\n",
      "[[47 19]\n",
      " [22 32]]\n",
      "Accuracy on test data:0.658333333333\n",
      "Accuracy on training data:1.0\n"
     ]
    }
   ],
   "source": [
    "bank_data_knn = KNeighborsClassifier(n_neighbors = 11, weights = 'distance',metric='euclidean')\n",
    "bank_data_knn.fit(df_train_norm, df_target_train)\n",
    "df_target_predict = bank_data_knn.predict(df_test_norm)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_target_test, df_target_predict))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "knn_confusion_matrix = confusion_matrix(y_true = df_target_test, y_pred = df_target_predict)\n",
    "print(knn_confusion_matrix)\n",
    "\n",
    "print(\"Accuracy on test data:\" + str(bank_data_knn.score(df_test_norm, df_target_test)))\n",
    "print(\"Accuracy on training data:\" + str(bank_data_knn.score(df_train_norm, df_target_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         NO       0.64      0.67      0.65        66\n",
      "        YES       0.57      0.54      0.55        54\n",
      "\n",
      "avg / total       0.61      0.61      0.61       120\n",
      "\n",
      "[[44 22]\n",
      " [25 29]]\n",
      "Accuracy on test data:0.608333333333\n",
      "Accuracy on training data:1.0\n"
     ]
    }
   ],
   "source": [
    "bank_data_knn = KNeighborsClassifier(n_neighbors = 13, weights = 'distance',metric='euclidean')\n",
    "bank_data_knn.fit(df_train_norm, df_target_train)\n",
    "df_target_predict = bank_data_knn.predict(df_test_norm)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_target_test, df_target_predict))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "knn_confusion_matrix = confusion_matrix(y_true = df_target_test, y_pred = df_target_predict)\n",
    "print(knn_confusion_matrix)\n",
    "\n",
    "print(\"Accuracy on test data:\" + str(bank_data_knn.score(df_test_norm, df_target_test)))\n",
    "print(\"Accuracy on training data:\" + str(bank_data_knn.score(df_train_norm, df_target_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         NO       0.64      0.67      0.65        66\n",
      "        YES       0.57      0.54      0.55        54\n",
      "\n",
      "avg / total       0.61      0.61      0.61       120\n",
      "\n",
      "[[44 22]\n",
      " [25 29]]\n",
      "Accuracy on test data:0.608333333333\n",
      "Accuracy on training data:1.0\n"
     ]
    }
   ],
   "source": [
    "bank_data_knn = KNeighborsClassifier(n_neighbors = 15, weights = 'distance', metric='euclidean')\n",
    "bank_data_knn.fit(df_train_norm, df_target_train)\n",
    "df_target_predict = bank_data_knn.predict(df_test_norm)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_target_test, df_target_predict))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "knn_confusion_matrix = confusion_matrix(y_true = df_target_test, y_pred = df_target_predict)\n",
    "print(knn_confusion_matrix)\n",
    "\n",
    "print(\"Accuracy on test data:\" + str(bank_data_knn.score(df_test_norm, df_target_test)))\n",
    "print(\"Accuracy on training data:\" + str(bank_data_knn.score(df_train_norm, df_target_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the value of k in the model parameters we can see that the maximum accuracy we achieve on test data is 65.08% or 0.6583 for k = 11 and then it starts decreasing if we increase the value of k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the weights parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         NO       0.67      0.70      0.68        66\n",
      "        YES       0.61      0.57      0.59        54\n",
      "\n",
      "avg / total       0.64      0.64      0.64       120\n",
      "\n",
      "[[46 20]\n",
      " [23 31]]\n",
      "Accuracy on test data:0.641666666667\n",
      "Accuracy on training data:0.783333333333\n"
     ]
    }
   ],
   "source": [
    "bank_data_knn = KNeighborsClassifier(n_neighbors = 5, weights = 'uniform', metric='euclidean')\n",
    "bank_data_knn.fit(df_train_norm, df_target_train)\n",
    "df_target_predict = bank_data_knn.predict(df_test_norm)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_target_test, df_target_predict))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "knn_confusion_matrix = confusion_matrix(y_true = df_target_test, y_pred = df_target_predict)\n",
    "print(knn_confusion_matrix)\n",
    "\n",
    "print(\"Accuracy on test data:\" + str(bank_data_knn.score(df_test_norm, df_target_test)))\n",
    "print(\"Accuracy on training data:\" + str(bank_data_knn.score(df_train_norm, df_target_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         NO       0.69      0.73      0.71        66\n",
      "        YES       0.64      0.59      0.62        54\n",
      "\n",
      "avg / total       0.67      0.67      0.67       120\n",
      "\n",
      "[[48 18]\n",
      " [22 32]]\n",
      "Accuracy on test data:0.666666666667\n",
      "Accuracy on training data:0.777083333333\n"
     ]
    }
   ],
   "source": [
    "bank_data_knn = KNeighborsClassifier(n_neighbors = 7, weights = 'uniform', metric='euclidean')\n",
    "bank_data_knn.fit(df_train_norm, df_target_train)\n",
    "df_target_predict = bank_data_knn.predict(df_test_norm)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_target_test, df_target_predict))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "knn_confusion_matrix = confusion_matrix(y_true = df_target_test, y_pred = df_target_predict)\n",
    "print(knn_confusion_matrix)\n",
    "\n",
    "print(\"Accuracy on test data:\" + str(bank_data_knn.score(df_test_norm, df_target_test)))\n",
    "print(\"Accuracy on training data:\" + str(bank_data_knn.score(df_train_norm, df_target_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         NO       0.69      0.77      0.73        66\n",
      "        YES       0.67      0.57      0.62        54\n",
      "\n",
      "avg / total       0.68      0.68      0.68       120\n",
      "\n",
      "[[51 15]\n",
      " [23 31]]\n",
      "Accuracy on test data:0.683333333333\n",
      "Accuracy on training data:0.770833333333\n"
     ]
    }
   ],
   "source": [
    "bank_data_knn = KNeighborsClassifier(n_neighbors = 9, weights = 'uniform', metric='euclidean')\n",
    "bank_data_knn.fit(df_train_norm, df_target_train)\n",
    "df_target_predict = bank_data_knn.predict(df_test_norm)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_target_test, df_target_predict))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "knn_confusion_matrix = confusion_matrix(y_true = df_target_test, y_pred = df_target_predict)\n",
    "print(knn_confusion_matrix)\n",
    "\n",
    "print(\"Accuracy on test data:\" + str(bank_data_knn.score(df_test_norm, df_target_test)))\n",
    "print(\"Accuracy on training data:\" + str(bank_data_knn.score(df_train_norm, df_target_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         NO       0.66      0.71      0.69        66\n",
      "        YES       0.61      0.56      0.58        54\n",
      "\n",
      "avg / total       0.64      0.64      0.64       120\n",
      "\n",
      "[[47 19]\n",
      " [24 30]]\n",
      "Accuracy on test data:0.641666666667\n",
      "Accuracy on training data:0.770833333333\n"
     ]
    }
   ],
   "source": [
    "bank_data_knn = KNeighborsClassifier(n_neighbors = 11, weights = 'uniform', metric='euclidean')\n",
    "bank_data_knn.fit(df_train_norm, df_target_train)\n",
    "df_target_predict = bank_data_knn.predict(df_test_norm)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_target_test, df_target_predict))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "knn_confusion_matrix = confusion_matrix(y_true = df_target_test, y_pred = df_target_predict)\n",
    "print(knn_confusion_matrix)\n",
    "\n",
    "print(\"Accuracy on test data:\" + str(bank_data_knn.score(df_test_norm, df_target_test)))\n",
    "print(\"Accuracy on training data:\" + str(bank_data_knn.score(df_train_norm, df_target_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         NO       0.66      0.71      0.69        66\n",
      "        YES       0.61      0.56      0.58        54\n",
      "\n",
      "avg / total       0.64      0.64      0.64       120\n",
      "\n",
      "[[47 19]\n",
      " [24 30]]\n",
      "Accuracy on test data:0.641666666667\n",
      "Accuracy on training data:0.745833333333\n"
     ]
    }
   ],
   "source": [
    "bank_data_knn = KNeighborsClassifier(n_neighbors = 13, weights = 'uniform', metric='euclidean')\n",
    "bank_data_knn.fit(df_train_norm, df_target_train)\n",
    "df_target_predict = bank_data_knn.predict(df_test_norm)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_target_test, df_target_predict))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "knn_confusion_matrix = confusion_matrix(y_true = df_target_test, y_pred = df_target_predict)\n",
    "print(knn_confusion_matrix)\n",
    "\n",
    "print(\"Accuracy on test data:\" + str(bank_data_knn.score(df_test_norm, df_target_test)))\n",
    "print(\"Accuracy on training data:\" + str(bank_data_knn.score(df_train_norm, df_target_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         NO       0.67      0.68      0.68        66\n",
      "        YES       0.60      0.59      0.60        54\n",
      "\n",
      "avg / total       0.64      0.64      0.64       120\n",
      "\n",
      "[[45 21]\n",
      " [22 32]]\n",
      "Accuracy on test data:0.641666666667\n",
      "Accuracy on training data:0.735416666667\n"
     ]
    }
   ],
   "source": [
    "bank_data_knn = KNeighborsClassifier(n_neighbors = 15, weights = 'uniform', metric='euclidean')\n",
    "bank_data_knn.fit(df_train_norm, df_target_train)\n",
    "df_target_predict = bank_data_knn.predict(df_test_norm)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_target_test, df_target_predict))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "knn_confusion_matrix = confusion_matrix(y_true = df_target_test, y_pred = df_target_predict)\n",
    "print(knn_confusion_matrix)\n",
    "\n",
    "print(\"Accuracy on test data:\" + str(bank_data_knn.score(df_test_norm, df_target_test)))\n",
    "print(\"Accuracy on training data:\" + str(bank_data_knn.score(df_train_norm, df_target_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the value of weights in the model parameters we can see that the maximum accuracy we achieve on test data is 68.33% or 0.6833 for k = 9 and then it starts decreasing. Also changing the weights to \"uniform\" improves precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the metric parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
